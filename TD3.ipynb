{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPX2Nehm26UoefYTLfMIYdt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ofredy/Twin-Delayed-DDPG/blob/main/TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybullet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4av4DfN2AN7",
        "outputId": "d6a22152-c21a-46e5-be7c-f2d37dc96002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybullet\n",
            "  Downloading pybullet-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (103.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybullet\n",
            "Successfully installed pybullet-3.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk5kVob31Z_X",
        "outputId": "120a19b1-f115-4ba5-824e-5391cabed651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:440: UserWarning: \u001b[33mWARN: The `registry.env_specs` property along with `EnvSpecTree` is deprecated. Please use `registry` directly as a dictionary instead.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pybullet_envs\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from gym import wrappers\n",
        "from torch.autograd import Variable\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Initialize the Experience Replay Memory\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "    def __init__(self, max_size=1e6):\n",
        "\n",
        "        self.storage = []\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "\n",
        "    def add(self, transition):\n",
        "\n",
        "        if len(self.storage) == self.max_size:\n",
        "            self.storage[int(self.ptr)] = transition\n",
        "            self.ptr = (self.ptr + 1) % self.max_size\n",
        "        else:\n",
        "            self.storage.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "\n",
        "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
        "        batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "\n",
        "        for i in ind:\n",
        "\n",
        "            state, next_state, action, reward, done = self.storage[i]\n",
        "\n",
        "            batch_states.append(np.array(state, copy=False))\n",
        "            batch_next_states.append(np.array(next_state, copy=False))\n",
        "            batch_actions.append(np.array(action, copy=False))\n",
        "            batch_rewards.append(np.array(reward, copy=False))\n",
        "            batch_dones.append(np.array(done, copy=False))\n",
        "\n",
        "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rn30mTbV2XvY",
        "outputId": "35bc3b3b-1eea-485c-e02d-7f5ef108a3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: We build one nn for the Actor Model and one nn for the Actor target\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.layer_1 = nn.Linear(state_dim, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "aeTB-8kb2kVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build two nn for the two critic models and two critic targets\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # First critic nn\n",
        "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, 1)\n",
        "\n",
        "        # Second critic nn\n",
        "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.layer_5 = nn.Linear(400, 300)\n",
        "        self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "\n",
        "        xu = torch.cat([x, u], axis=1)\n",
        "\n",
        "        # Forward prop twin 1\n",
        "        x1 = F.relu(self.layer_1(xu))\n",
        "        x1 = F.relu(self.layer_2(x1))\n",
        "        x1 = self.layer_3(x1)\n",
        "\n",
        "        # Forward prop twin 2\n",
        "        x2 = F.relu(self.layer_4(xu))\n",
        "        x2 = F.relu(self.layer_5(x2))\n",
        "        x2 = self.layer_6(x2)\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "\n",
        "        xu = torch.cat([x, u], axis=1)\n",
        "\n",
        "        # Forward prop twin 1\n",
        "        x1 = F.relu(self.layer_1(xu))\n",
        "        x1 = F.relu(self.layer_2(x1))\n",
        "        x1 = self.layer_3(x1)\n",
        "\n",
        "        return x1"
      ],
      "metadata": {
        "id": "wjOSuzS0AsJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Steps 4 to 15: Training Process\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole training process into a class\n",
        "class TD3(object):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "\n",
        "        # Defining the actor model and target\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "\n",
        "        # Defining the twin critic models and targets\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "        for it in range(iterations):\n",
        "\n",
        "            # Step 4: Sample a batch of transitions (s, s', a, r) from the memory\n",
        "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "            state = torch.Tensor(batch_states).to(device)\n",
        "            next_state = torch.Tensor(batch_next_states).to(device)\n",
        "            action = torch.Tensor(batch_actions).to(device)\n",
        "            reward = torch.Tensor(batch_rewards).to(device)\n",
        "            done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "            # Step 5: From the next state s', the Actor target plays the next action a'\n",
        "            next_action = self.actor_target(next_state)\n",
        "\n",
        "            # Step 6: Add Gaussian noise to this next action a' and we clamp it in a range of values supported by the environment\n",
        "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Step 7: The two Critic targets take each the couple (s', a') as input and return two Q-values\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "            # Step 8: Keep the minimum of the two Q-values\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "            # Step 9: Get the final target of the two Critic models, which is Qt = r + y * min(Qt1, Qt2), where y is the discount factor\n",
        "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "            # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values\n",
        "            current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "            # Step 11: Compute the loss coming from the two Critic models: Critic_Loss = MSE(Q1, Qt) + MSE(Q2, Qt)\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "            # Step 12: Backprop the critic_loss and update the parameters of the two critic models\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # Step 13-15:\n",
        "            if it % policy_freq:\n",
        "\n",
        "                # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first crictic model\n",
        "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Step 14: Once every two iterations, update the weights of the Actor target by polyak avergaing\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                # Step 15: Once every two iterations, update the weights of the Critic target by polyak avergaing\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    # Making a save method to save a trained model\n",
        "    def save(self, filename, directory):\n",
        "      torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "      torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "    # Making a load method to load a pre-trained model\n",
        "    def load(self, filename, directory):\n",
        "      self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "      self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "metadata": {
        "id": "94GwynP3RR9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a function that evaluates the policy by calculating its avg reward over 10 episode\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "\n",
        "    avg_reward = 0.\n",
        "\n",
        "    for _ in range(eval_episodes):\n",
        "\n",
        "      obs = env.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "        action = policy.select_action(np.array(obs))\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        avg_reward += reward\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "\n",
        "    print (\"---------------------------------------\")\n",
        "    print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "    print (\"---------------------------------------\")\n",
        "\n",
        "    return avg_reward"
      ],
      "metadata": {
        "id": "os_blxm8NfB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the parameters\n",
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "metadata": {
        "id": "N3eG3OjWOlZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a file name for the two saved models: The actor and critic models\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CE3Z6NqOzNi",
        "outputId": "32dd8a7b-855f-4f43-ab0a-4feaf113f1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a folder inside which will be save the trained models\n",
        "if not os.path.exists(\"./results\"):\n",
        "  os.makedirs(\"./results\")\n",
        "if save_models and not os.path.exists(\"./pytorch_models\"):\n",
        "  os.makedirs(\"./pytorch_models\")"
      ],
      "metadata": {
        "id": "yGWj0a1DPSCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the PyBullet environment\n",
        "env = gym.make(env_name)"
      ],
      "metadata": {
        "id": "CKJWpkfxPb0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get seeds and we get the necessary information on the states and actions in the chosen environment\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "metadata": {
        "id": "5QI7yFz8PtcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the policy network\n",
        "policy = TD3(state_dim, action_dim, max_action)"
      ],
      "metadata": {
        "id": "Rw6sKVyaP40D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Experience Replay Memory\n",
        "replay_buffer = ReplayBuffer()"
      ],
      "metadata": {
        "id": "8XVL43TtQWlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list where all the evalution results over 10 episodes are stored\n",
        "evaluations = [evaluate_policy]"
      ],
      "metadata": {
        "id": "tT5KlNpLQfMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new folder directory in which the final results will be populated\n",
        "def mkdir(base, name):\n",
        "\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "    return path\n",
        "\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "2djiIKvLQsTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the variables\n",
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()"
      ],
      "metadata": {
        "id": "h2H-Y4GAQ-Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "# We start the main loop over 500,000 timesteps\n",
        "while total_timesteps < max_timesteps:\n",
        "\n",
        "    # If the episode is done\n",
        "    if done:\n",
        "        # If we are not at the very beginning, we start the training process of the model\n",
        "        if total_timesteps != 0:\n",
        "            print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "            policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "        # We evaluate the episode and we save the policy\n",
        "        if timesteps_since_eval >= eval_freq:\n",
        "            timesteps_since_eval %= eval_freq\n",
        "            evaluations.append(evaluate_policy(policy))\n",
        "            policy.save(file_name, directory=\"./pytorch_models\")\n",
        "            np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "\n",
        "        # When the training step is done, we reset the state of the environment\n",
        "        obs = env.reset()\n",
        "\n",
        "        # Set the Done to False\n",
        "        done = False\n",
        "\n",
        "        # Set rewards and episode timesteps to zero\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "\n",
        "    # Before 10000 timesteps, we play random actions\n",
        "    if total_timesteps < start_timesteps:\n",
        "        action = env.action_space.sample()\n",
        "    else: # After 10000 timesteps, we switch to the model\n",
        "        action = policy.select_action(np.array(obs))\n",
        "        # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "        if expl_noise != 0:\n",
        "            action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "\n",
        "    # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "    new_obs, reward, done, _ = env.step(action)\n",
        "\n",
        "    # We check if the episode is done\n",
        "    done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "\n",
        "    # We increase the total reward\n",
        "    episode_reward += reward\n",
        "\n",
        "    # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "    replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "    # We update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "    obs = new_obs\n",
        "    episode_timesteps += 1\n",
        "    total_timesteps += 1\n",
        "    timesteps_since_eval += 1\n",
        "\n",
        "# We add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WPyF0kORH9k",
        "outputId": "84856650-4b18-43d6-fb03-5a018e3216a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 523.414014092835\n",
            "Total Timesteps: 2000 Episode Num: 2 Reward: 498.53493101184984\n",
            "Total Timesteps: 3000 Episode Num: 3 Reward: 514.9351468428981\n",
            "Total Timesteps: 4000 Episode Num: 4 Reward: 408.8117780190493\n",
            "Total Timesteps: 5000 Episode Num: 5 Reward: 515.0530742753795\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 135.639130\n",
            "---------------------------------------\n",
            "Total Timesteps: 5044 Episode Num: 6 Reward: 17.099061612001965\n",
            "Total Timesteps: 6044 Episode Num: 7 Reward: 506.1839058793301\n",
            "Total Timesteps: 6337 Episode Num: 8 Reward: 144.02830974502584\n",
            "Total Timesteps: 6518 Episode Num: 9 Reward: 91.65253600926951\n",
            "Total Timesteps: 7518 Episode Num: 10 Reward: 505.9350834770443\n",
            "Total Timesteps: 7752 Episode Num: 11 Reward: 116.0769383492414\n",
            "Total Timesteps: 8752 Episode Num: 12 Reward: 510.92942453097965\n",
            "Total Timesteps: 9752 Episode Num: 13 Reward: 551.8891159622035\n",
            "Total Timesteps: 9941 Episode Num: 14 Reward: 82.39872202534316\n",
            "Total Timesteps: 10941 Episode Num: 15 Reward: 263.1549598019008\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 166.874669\n",
            "---------------------------------------\n",
            "Total Timesteps: 11941 Episode Num: 16 Reward: 190.8510175255993\n",
            "Total Timesteps: 12941 Episode Num: 17 Reward: 110.10375306263721\n",
            "Total Timesteps: 13941 Episode Num: 18 Reward: 199.05737243821642\n",
            "Total Timesteps: 14941 Episode Num: 19 Reward: 191.05843656425066\n",
            "Total Timesteps: 15941 Episode Num: 20 Reward: 298.9847918067057\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 198.929323\n",
            "---------------------------------------\n",
            "Total Timesteps: 16941 Episode Num: 21 Reward: 208.40207377983563\n",
            "Total Timesteps: 17941 Episode Num: 22 Reward: 313.14089620616346\n",
            "Total Timesteps: 18941 Episode Num: 23 Reward: 264.5256546447691\n",
            "Total Timesteps: 19941 Episode Num: 24 Reward: 475.8237383434478\n",
            "Total Timesteps: 20941 Episode Num: 25 Reward: 378.26776862136524\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 280.390876\n",
            "---------------------------------------\n",
            "Total Timesteps: 21260 Episode Num: 26 Reward: 113.3517574687349\n",
            "Total Timesteps: 22260 Episode Num: 27 Reward: 390.37065347247625\n",
            "Total Timesteps: 23260 Episode Num: 28 Reward: 339.1260702438557\n",
            "Total Timesteps: 24260 Episode Num: 29 Reward: 570.6352938317627\n",
            "Total Timesteps: 25260 Episode Num: 30 Reward: 404.6074587736454\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 304.810414\n",
            "---------------------------------------\n",
            "Total Timesteps: 26260 Episode Num: 31 Reward: 146.80237988489696\n",
            "Total Timesteps: 27260 Episode Num: 32 Reward: 335.5139558014517\n",
            "Total Timesteps: 28260 Episode Num: 33 Reward: 224.52667810067146\n",
            "Total Timesteps: 28452 Episode Num: 34 Reward: -35.51103046948381\n",
            "Total Timesteps: 28595 Episode Num: 35 Reward: 63.38395458817104\n",
            "Total Timesteps: 29595 Episode Num: 36 Reward: 280.5061999925837\n",
            "Total Timesteps: 30595 Episode Num: 37 Reward: 233.7909521880432\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 389.567514\n",
            "---------------------------------------\n",
            "Total Timesteps: 31595 Episode Num: 38 Reward: 466.0216145282132\n",
            "Total Timesteps: 32595 Episode Num: 39 Reward: 370.4471321253982\n",
            "Total Timesteps: 33595 Episode Num: 40 Reward: 268.95607014487683\n",
            "Total Timesteps: 34595 Episode Num: 41 Reward: 527.4955063446174\n",
            "Total Timesteps: 35595 Episode Num: 42 Reward: 319.8684025585837\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 421.938842\n",
            "---------------------------------------\n",
            "Total Timesteps: 36595 Episode Num: 43 Reward: 255.10995921236523\n",
            "Total Timesteps: 36779 Episode Num: 44 Reward: 143.84645905850604\n",
            "Total Timesteps: 37415 Episode Num: 45 Reward: 273.8696591256618\n",
            "Total Timesteps: 37713 Episode Num: 46 Reward: 147.42558527306156\n",
            "Total Timesteps: 38713 Episode Num: 47 Reward: 843.8026263621814\n",
            "Total Timesteps: 39713 Episode Num: 48 Reward: 216.29755503334496\n",
            "Total Timesteps: 40713 Episode Num: 49 Reward: 706.9744047679757\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 251.092705\n",
            "---------------------------------------\n",
            "Total Timesteps: 41713 Episode Num: 50 Reward: 418.57221947666994\n",
            "Total Timesteps: 42713 Episode Num: 51 Reward: 488.523517829457\n",
            "Total Timesteps: 43713 Episode Num: 52 Reward: 283.2919280529369\n",
            "Total Timesteps: 44713 Episode Num: 53 Reward: 302.9042237852024\n",
            "Total Timesteps: 44846 Episode Num: 54 Reward: 54.43684678347339\n",
            "Total Timesteps: 45846 Episode Num: 55 Reward: 321.90654776206065\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 411.436381\n",
            "---------------------------------------\n",
            "Total Timesteps: 46846 Episode Num: 56 Reward: 624.7843223849687\n",
            "Total Timesteps: 47846 Episode Num: 57 Reward: 466.30765781073\n",
            "Total Timesteps: 48846 Episode Num: 58 Reward: 541.6579032361178\n",
            "Total Timesteps: 49846 Episode Num: 59 Reward: 379.03545958496534\n",
            "Total Timesteps: 50846 Episode Num: 60 Reward: 620.6963547047267\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 395.388281\n",
            "---------------------------------------\n",
            "Total Timesteps: 51846 Episode Num: 61 Reward: 414.81795417239596\n",
            "Total Timesteps: 52846 Episode Num: 62 Reward: 356.8317044817737\n",
            "Total Timesteps: 53846 Episode Num: 63 Reward: 417.6760243519654\n",
            "Total Timesteps: 53866 Episode Num: 64 Reward: 2.019195018152857\n",
            "Total Timesteps: 53887 Episode Num: 65 Reward: 3.2278133149101618\n",
            "Total Timesteps: 54887 Episode Num: 66 Reward: 651.6527708780188\n",
            "Total Timesteps: 55887 Episode Num: 67 Reward: 451.5662031468959\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 281.265651\n",
            "---------------------------------------\n",
            "Total Timesteps: 56887 Episode Num: 68 Reward: 511.6140048660464\n",
            "Total Timesteps: 57887 Episode Num: 69 Reward: 199.77991737079407\n",
            "Total Timesteps: 58887 Episode Num: 70 Reward: 271.04130955228\n",
            "Total Timesteps: 59887 Episode Num: 71 Reward: 263.675017350542\n",
            "Total Timesteps: 60887 Episode Num: 72 Reward: 215.10341997720352\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 353.369521\n",
            "---------------------------------------\n",
            "Total Timesteps: 61887 Episode Num: 73 Reward: 539.3551788167943\n",
            "Total Timesteps: 62887 Episode Num: 74 Reward: 315.87597453706485\n",
            "Total Timesteps: 63887 Episode Num: 75 Reward: 433.56062336118543\n",
            "Total Timesteps: 64887 Episode Num: 76 Reward: 272.73447204100984\n",
            "Total Timesteps: 65887 Episode Num: 77 Reward: 311.4562332901011\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 385.299055\n",
            "---------------------------------------\n",
            "Total Timesteps: 66887 Episode Num: 78 Reward: 372.321670802106\n",
            "Total Timesteps: 67887 Episode Num: 79 Reward: 471.62843548417106\n",
            "Total Timesteps: 68887 Episode Num: 80 Reward: 258.43225690800483\n",
            "Total Timesteps: 69887 Episode Num: 81 Reward: 423.24813849774534\n",
            "Total Timesteps: 70887 Episode Num: 82 Reward: 129.5980101168338\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 329.178470\n",
            "---------------------------------------\n",
            "Total Timesteps: 71887 Episode Num: 83 Reward: 301.3429233746069\n",
            "Total Timesteps: 72887 Episode Num: 84 Reward: 378.4496464014653\n",
            "Total Timesteps: 73887 Episode Num: 85 Reward: 164.75527105799614\n",
            "Total Timesteps: 74887 Episode Num: 86 Reward: 423.3403478091603\n",
            "Total Timesteps: 75887 Episode Num: 87 Reward: 530.7167600955679\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 360.219178\n",
            "---------------------------------------\n",
            "Total Timesteps: 76887 Episode Num: 88 Reward: 285.17158644785883\n",
            "Total Timesteps: 77887 Episode Num: 89 Reward: 360.38467179832145\n",
            "Total Timesteps: 78887 Episode Num: 90 Reward: 330.3511729895122\n",
            "Total Timesteps: 79887 Episode Num: 91 Reward: 475.2340363056228\n",
            "Total Timesteps: 80887 Episode Num: 92 Reward: 294.2522819590257\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 558.957785\n",
            "---------------------------------------\n",
            "Total Timesteps: 81887 Episode Num: 93 Reward: 523.5642489233601\n",
            "Total Timesteps: 82887 Episode Num: 94 Reward: 585.7236018498779\n",
            "Total Timesteps: 83887 Episode Num: 95 Reward: 611.1152169054761\n",
            "Total Timesteps: 84887 Episode Num: 96 Reward: 718.510137176214\n",
            "Total Timesteps: 85887 Episode Num: 97 Reward: 544.0652877570138\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 539.018664\n",
            "---------------------------------------\n",
            "Total Timesteps: 86887 Episode Num: 98 Reward: 418.6059764849154\n",
            "Total Timesteps: 87887 Episode Num: 99 Reward: 650.3641283369582\n",
            "Total Timesteps: 87973 Episode Num: 100 Reward: -39.525077110609686\n",
            "Total Timesteps: 88973 Episode Num: 101 Reward: 461.1315060137661\n",
            "Total Timesteps: 89942 Episode Num: 102 Reward: 512.4714953541425\n",
            "Total Timesteps: 90942 Episode Num: 103 Reward: 595.6167000320215\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 688.157349\n",
            "---------------------------------------\n",
            "Total Timesteps: 91942 Episode Num: 104 Reward: 760.7090926170948\n",
            "Total Timesteps: 92942 Episode Num: 105 Reward: 553.9916058491978\n",
            "Total Timesteps: 93942 Episode Num: 106 Reward: 504.9510333150202\n",
            "Total Timesteps: 94942 Episode Num: 107 Reward: 674.3981111736485\n",
            "Total Timesteps: 95942 Episode Num: 108 Reward: 405.57554736347646\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 482.308880\n",
            "---------------------------------------\n",
            "Total Timesteps: 96942 Episode Num: 109 Reward: 447.8674199886868\n",
            "Total Timesteps: 97942 Episode Num: 110 Reward: 654.4493161474234\n",
            "Total Timesteps: 98942 Episode Num: 111 Reward: 658.0671418883222\n",
            "Total Timesteps: 99942 Episode Num: 112 Reward: 497.8771855875919\n",
            "Total Timesteps: 100942 Episode Num: 113 Reward: 651.0476017108101\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 538.696413\n",
            "---------------------------------------\n",
            "Total Timesteps: 101942 Episode Num: 114 Reward: 626.378907901613\n",
            "Total Timesteps: 102942 Episode Num: 115 Reward: 628.0329605990173\n",
            "Total Timesteps: 103942 Episode Num: 116 Reward: 640.501265128678\n",
            "Total Timesteps: 104942 Episode Num: 117 Reward: 781.5541736502516\n",
            "Total Timesteps: 105942 Episode Num: 118 Reward: 461.8126954527126\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 531.432217\n",
            "---------------------------------------\n",
            "Total Timesteps: 106942 Episode Num: 119 Reward: 544.343356533088\n",
            "Total Timesteps: 107942 Episode Num: 120 Reward: 304.5219438748392\n",
            "Total Timesteps: 108942 Episode Num: 121 Reward: 530.4426701471832\n",
            "Total Timesteps: 109942 Episode Num: 122 Reward: 518.8024225866761\n",
            "Total Timesteps: 110942 Episode Num: 123 Reward: 380.11746754165824\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 560.510662\n",
            "---------------------------------------\n",
            "Total Timesteps: 111942 Episode Num: 124 Reward: 527.3423876392909\n",
            "Total Timesteps: 112942 Episode Num: 125 Reward: 614.7913778894051\n",
            "Total Timesteps: 113942 Episode Num: 126 Reward: 702.3521224324795\n",
            "Total Timesteps: 114942 Episode Num: 127 Reward: 314.7443611236478\n",
            "Total Timesteps: 115942 Episode Num: 128 Reward: 512.5342703029312\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 523.778863\n",
            "---------------------------------------\n",
            "Total Timesteps: 116942 Episode Num: 129 Reward: 605.6238080595742\n",
            "Total Timesteps: 117942 Episode Num: 130 Reward: 338.2745268382262\n",
            "Total Timesteps: 118942 Episode Num: 131 Reward: 572.1100333828296\n",
            "Total Timesteps: 119942 Episode Num: 132 Reward: 779.5894639574026\n",
            "Total Timesteps: 120942 Episode Num: 133 Reward: 732.7200024140707\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 741.380759\n",
            "---------------------------------------\n",
            "Total Timesteps: 121942 Episode Num: 134 Reward: 684.855030770657\n",
            "Total Timesteps: 122942 Episode Num: 135 Reward: 752.1566825527822\n",
            "Total Timesteps: 123942 Episode Num: 136 Reward: 491.86226175871843\n",
            "Total Timesteps: 124942 Episode Num: 137 Reward: 677.553410752495\n",
            "Total Timesteps: 125942 Episode Num: 138 Reward: 775.3609723450174\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 801.664817\n",
            "---------------------------------------\n",
            "Total Timesteps: 126942 Episode Num: 139 Reward: 715.4927513896118\n",
            "Total Timesteps: 127942 Episode Num: 140 Reward: 741.459983056227\n",
            "Total Timesteps: 128942 Episode Num: 141 Reward: 641.7913533011192\n",
            "Total Timesteps: 129942 Episode Num: 142 Reward: 842.8999055041446\n",
            "Total Timesteps: 130942 Episode Num: 143 Reward: 669.3096538941262\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 602.450552\n",
            "---------------------------------------\n",
            "Total Timesteps: 131942 Episode Num: 144 Reward: 661.4360799062849\n",
            "Total Timesteps: 132942 Episode Num: 145 Reward: 738.1694431464368\n",
            "Total Timesteps: 133942 Episode Num: 146 Reward: 753.1551191523279\n",
            "Total Timesteps: 134942 Episode Num: 147 Reward: 891.0791604729175\n",
            "Total Timesteps: 135942 Episode Num: 148 Reward: 672.8160810872529\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 698.533131\n",
            "---------------------------------------\n",
            "Total Timesteps: 136942 Episode Num: 149 Reward: 586.1897172875075\n",
            "Total Timesteps: 137942 Episode Num: 150 Reward: 660.8846871250233\n",
            "Total Timesteps: 138942 Episode Num: 151 Reward: 804.3436576988773\n",
            "Total Timesteps: 139942 Episode Num: 152 Reward: 747.2882016824402\n",
            "Total Timesteps: 140942 Episode Num: 153 Reward: 681.9323769933644\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 598.444700\n",
            "---------------------------------------\n",
            "Total Timesteps: 141942 Episode Num: 154 Reward: 518.6337434739006\n",
            "Total Timesteps: 142942 Episode Num: 155 Reward: 690.8913195922254\n",
            "Total Timesteps: 143942 Episode Num: 156 Reward: 674.6392391036312\n",
            "Total Timesteps: 144942 Episode Num: 157 Reward: 743.5382385211445\n",
            "Total Timesteps: 145942 Episode Num: 158 Reward: 671.191485222869\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 603.803362\n",
            "---------------------------------------\n",
            "Total Timesteps: 146942 Episode Num: 159 Reward: 665.5510739832307\n",
            "Total Timesteps: 147942 Episode Num: 160 Reward: 597.8474673296475\n",
            "Total Timesteps: 148942 Episode Num: 161 Reward: 487.8744892284124\n",
            "Total Timesteps: 149942 Episode Num: 162 Reward: 774.1949071589423\n",
            "Total Timesteps: 150942 Episode Num: 163 Reward: 764.1606975326791\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 513.421527\n",
            "---------------------------------------\n",
            "Total Timesteps: 151942 Episode Num: 164 Reward: 458.64545772131305\n",
            "Total Timesteps: 152942 Episode Num: 165 Reward: 471.90208290749376\n",
            "Total Timesteps: 153942 Episode Num: 166 Reward: 522.9886615275011\n",
            "Total Timesteps: 154942 Episode Num: 167 Reward: 655.505979184909\n",
            "Total Timesteps: 155942 Episode Num: 168 Reward: 627.6689325995864\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 615.535746\n",
            "---------------------------------------\n",
            "Total Timesteps: 156942 Episode Num: 169 Reward: 318.20244974759925\n",
            "Total Timesteps: 157942 Episode Num: 170 Reward: 545.3647561610185\n",
            "Total Timesteps: 158942 Episode Num: 171 Reward: 644.5039982929502\n",
            "Total Timesteps: 159942 Episode Num: 172 Reward: 412.71007987042543\n",
            "Total Timesteps: 160942 Episode Num: 173 Reward: 717.3331143697453\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 589.702222\n",
            "---------------------------------------\n",
            "Total Timesteps: 161942 Episode Num: 174 Reward: 456.62967336464743\n",
            "Total Timesteps: 162942 Episode Num: 175 Reward: 529.43827687755\n",
            "Total Timesteps: 163942 Episode Num: 176 Reward: 711.2161590283237\n",
            "Total Timesteps: 164942 Episode Num: 177 Reward: 633.2470255391295\n",
            "Total Timesteps: 165942 Episode Num: 178 Reward: 710.4253526220086\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 624.588964\n",
            "---------------------------------------\n",
            "Total Timesteps: 166942 Episode Num: 179 Reward: 696.4085600941588\n",
            "Total Timesteps: 167942 Episode Num: 180 Reward: 657.6171399064141\n",
            "Total Timesteps: 168942 Episode Num: 181 Reward: 659.0732098390088\n",
            "Total Timesteps: 169942 Episode Num: 182 Reward: 771.5268149168234\n",
            "Total Timesteps: 170942 Episode Num: 183 Reward: 552.7816875508736\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 722.428139\n",
            "---------------------------------------\n",
            "Total Timesteps: 171942 Episode Num: 184 Reward: 727.3806968612233\n",
            "Total Timesteps: 172942 Episode Num: 185 Reward: 400.3097475337761\n",
            "Total Timesteps: 173942 Episode Num: 186 Reward: 753.0953225915881\n",
            "Total Timesteps: 174942 Episode Num: 187 Reward: 732.7412359387945\n",
            "Total Timesteps: 175942 Episode Num: 188 Reward: 821.8889402102034\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 605.406010\n",
            "---------------------------------------\n",
            "Total Timesteps: 176942 Episode Num: 189 Reward: 525.8900116320041\n",
            "Total Timesteps: 177942 Episode Num: 190 Reward: 710.03372758262\n",
            "Total Timesteps: 178942 Episode Num: 191 Reward: 722.0856044745268\n",
            "Total Timesteps: 179942 Episode Num: 192 Reward: 858.3088770816225\n",
            "Total Timesteps: 180942 Episode Num: 193 Reward: 671.4767567516049\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 635.734647\n",
            "---------------------------------------\n",
            "Total Timesteps: 181942 Episode Num: 194 Reward: 799.3052154655315\n",
            "Total Timesteps: 182942 Episode Num: 195 Reward: 616.433204411488\n",
            "Total Timesteps: 183942 Episode Num: 196 Reward: 610.1779990991832\n",
            "Total Timesteps: 184942 Episode Num: 197 Reward: 734.1041471803342\n",
            "Total Timesteps: 185942 Episode Num: 198 Reward: 806.2424261293455\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 718.426901\n",
            "---------------------------------------\n",
            "Total Timesteps: 186942 Episode Num: 199 Reward: 738.9377908152369\n",
            "Total Timesteps: 187942 Episode Num: 200 Reward: 681.0833305068654\n",
            "Total Timesteps: 188942 Episode Num: 201 Reward: 410.8764026427621\n",
            "Total Timesteps: 189942 Episode Num: 202 Reward: 734.9609654547615\n",
            "Total Timesteps: 190942 Episode Num: 203 Reward: 711.4669782158146\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 581.755465\n",
            "---------------------------------------\n",
            "Total Timesteps: 191942 Episode Num: 204 Reward: 468.2639899438466\n",
            "Total Timesteps: 192942 Episode Num: 205 Reward: 683.1479947576116\n",
            "Total Timesteps: 193942 Episode Num: 206 Reward: 546.1438945164009\n",
            "Total Timesteps: 194942 Episode Num: 207 Reward: 744.7005726127816\n",
            "Total Timesteps: 195942 Episode Num: 208 Reward: 762.4786690107635\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 690.612224\n",
            "---------------------------------------\n",
            "Total Timesteps: 196942 Episode Num: 209 Reward: 761.9673721790635\n",
            "Total Timesteps: 197942 Episode Num: 210 Reward: 759.2919046473093\n",
            "Total Timesteps: 198942 Episode Num: 211 Reward: 632.798218313204\n",
            "Total Timesteps: 199942 Episode Num: 212 Reward: 620.6021288372435\n",
            "Total Timesteps: 200942 Episode Num: 213 Reward: 820.403011669727\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1021.882252\n",
            "---------------------------------------\n",
            "Total Timesteps: 201942 Episode Num: 214 Reward: 1016.218766057282\n",
            "Total Timesteps: 202942 Episode Num: 215 Reward: 982.7999463272158\n",
            "Total Timesteps: 203942 Episode Num: 216 Reward: 962.7074608332474\n",
            "Total Timesteps: 204942 Episode Num: 217 Reward: 908.6649067575598\n",
            "Total Timesteps: 205942 Episode Num: 218 Reward: 971.8734251176644\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 846.469814\n",
            "---------------------------------------\n",
            "Total Timesteps: 206942 Episode Num: 219 Reward: 890.2178176805721\n",
            "Total Timesteps: 207942 Episode Num: 220 Reward: 945.6406673125064\n",
            "Total Timesteps: 208942 Episode Num: 221 Reward: 708.6366115586411\n",
            "Total Timesteps: 209942 Episode Num: 222 Reward: 894.6422578317245\n",
            "Total Timesteps: 210942 Episode Num: 223 Reward: 1131.450436872886\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 873.488723\n",
            "---------------------------------------\n",
            "Total Timesteps: 211942 Episode Num: 224 Reward: 1076.153498054321\n",
            "Total Timesteps: 212942 Episode Num: 225 Reward: 846.523358095532\n",
            "Total Timesteps: 213942 Episode Num: 226 Reward: 998.9181969600702\n",
            "Total Timesteps: 214942 Episode Num: 227 Reward: 782.0803856699686\n",
            "Total Timesteps: 215942 Episode Num: 228 Reward: 886.7775404570697\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 827.271277\n",
            "---------------------------------------\n",
            "Total Timesteps: 216942 Episode Num: 229 Reward: 832.816467662497\n",
            "Total Timesteps: 217942 Episode Num: 230 Reward: 791.0710216356003\n",
            "Total Timesteps: 218942 Episode Num: 231 Reward: 676.3751164603069\n",
            "Total Timesteps: 219942 Episode Num: 232 Reward: 562.4207234717566\n",
            "Total Timesteps: 220942 Episode Num: 233 Reward: 949.1856771219065\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 856.098296\n",
            "---------------------------------------\n",
            "Total Timesteps: 221942 Episode Num: 234 Reward: 956.5327985952848\n",
            "Total Timesteps: 222942 Episode Num: 235 Reward: 882.2292185563784\n",
            "Total Timesteps: 223942 Episode Num: 236 Reward: 1016.2362115712147\n",
            "Total Timesteps: 224942 Episode Num: 237 Reward: 585.4810193194807\n",
            "Total Timesteps: 225942 Episode Num: 238 Reward: 1153.5609868664003\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1105.209655\n",
            "---------------------------------------\n",
            "Total Timesteps: 226942 Episode Num: 239 Reward: 1022.8849403317352\n",
            "Total Timesteps: 227942 Episode Num: 240 Reward: 1182.167779186934\n",
            "Total Timesteps: 228942 Episode Num: 241 Reward: 998.0095007073706\n",
            "Total Timesteps: 229942 Episode Num: 242 Reward: 949.2184810494583\n",
            "Total Timesteps: 230942 Episode Num: 243 Reward: 766.1635050247127\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1187.107560\n",
            "---------------------------------------\n",
            "Total Timesteps: 231942 Episode Num: 244 Reward: 1297.8955171404\n",
            "Total Timesteps: 232942 Episode Num: 245 Reward: 1161.4569154673775\n",
            "Total Timesteps: 233942 Episode Num: 246 Reward: 1245.9609760413932\n",
            "Total Timesteps: 234942 Episode Num: 247 Reward: 1278.9637819023349\n",
            "Total Timesteps: 235942 Episode Num: 248 Reward: 1272.2068283291662\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1114.656121\n",
            "---------------------------------------\n",
            "Total Timesteps: 236942 Episode Num: 249 Reward: 1194.3816679548827\n",
            "Total Timesteps: 237942 Episode Num: 250 Reward: 1196.224494261037\n",
            "Total Timesteps: 238942 Episode Num: 251 Reward: 751.8819034230324\n",
            "Total Timesteps: 239942 Episode Num: 252 Reward: 1343.1801387568464\n",
            "Total Timesteps: 240942 Episode Num: 253 Reward: 1297.4474359039398\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1304.938527\n",
            "---------------------------------------\n",
            "Total Timesteps: 241942 Episode Num: 254 Reward: 1296.1068682741688\n",
            "Total Timesteps: 242942 Episode Num: 255 Reward: 1218.1462208295814\n",
            "Total Timesteps: 243942 Episode Num: 256 Reward: 978.054269417784\n",
            "Total Timesteps: 244942 Episode Num: 257 Reward: 1280.1132612735491\n",
            "Total Timesteps: 245942 Episode Num: 258 Reward: 974.1368157253811\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1142.462835\n",
            "---------------------------------------\n",
            "Total Timesteps: 246942 Episode Num: 259 Reward: 1239.3092086045824\n",
            "Total Timesteps: 247942 Episode Num: 260 Reward: 878.2658336861912\n",
            "Total Timesteps: 248942 Episode Num: 261 Reward: 943.0545808073027\n",
            "Total Timesteps: 249942 Episode Num: 262 Reward: 587.4844446468305\n",
            "Total Timesteps: 250942 Episode Num: 263 Reward: 1158.7792284756194\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 976.049198\n",
            "---------------------------------------\n",
            "Total Timesteps: 251942 Episode Num: 264 Reward: 1010.0193228365223\n",
            "Total Timesteps: 252942 Episode Num: 265 Reward: 1300.8356664954706\n",
            "Total Timesteps: 253942 Episode Num: 266 Reward: 1034.6057012282602\n",
            "Total Timesteps: 254942 Episode Num: 267 Reward: 1321.3675581149826\n",
            "Total Timesteps: 255942 Episode Num: 268 Reward: 1342.8986213620708\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1368.777408\n",
            "---------------------------------------\n",
            "Total Timesteps: 256942 Episode Num: 269 Reward: 1305.8268594207032\n",
            "Total Timesteps: 257942 Episode Num: 270 Reward: 590.032324148209\n",
            "Total Timesteps: 258942 Episode Num: 271 Reward: 1412.7953473355626\n",
            "Total Timesteps: 259942 Episode Num: 272 Reward: 1256.185316519787\n",
            "Total Timesteps: 260942 Episode Num: 273 Reward: 1263.9748243414608\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1076.501084\n",
            "---------------------------------------\n",
            "Total Timesteps: 261942 Episode Num: 274 Reward: 889.7751567381528\n",
            "Total Timesteps: 262942 Episode Num: 275 Reward: 1346.5747242774914\n",
            "Total Timesteps: 263942 Episode Num: 276 Reward: 1201.0598236055234\n",
            "Total Timesteps: 264942 Episode Num: 277 Reward: 1378.7441669951472\n",
            "Total Timesteps: 265942 Episode Num: 278 Reward: 1487.0839197668238\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1439.395964\n",
            "---------------------------------------\n",
            "Total Timesteps: 266942 Episode Num: 279 Reward: 1453.594787505142\n",
            "Total Timesteps: 267942 Episode Num: 280 Reward: 931.7415940526266\n",
            "Total Timesteps: 268942 Episode Num: 281 Reward: 966.3987846760838\n",
            "Total Timesteps: 269942 Episode Num: 282 Reward: 904.6054763438035\n",
            "Total Timesteps: 270942 Episode Num: 283 Reward: 1403.3059399221906\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1387.237623\n",
            "---------------------------------------\n",
            "Total Timesteps: 271942 Episode Num: 284 Reward: 1251.2956875698724\n",
            "Total Timesteps: 272942 Episode Num: 285 Reward: 1449.8368741355716\n",
            "Total Timesteps: 273942 Episode Num: 286 Reward: 1557.4024460704256\n",
            "Total Timesteps: 274942 Episode Num: 287 Reward: 1570.077777175528\n",
            "Total Timesteps: 275942 Episode Num: 288 Reward: 1361.176326142124\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1593.541918\n",
            "---------------------------------------\n",
            "Total Timesteps: 276942 Episode Num: 289 Reward: 1508.9933072462456\n",
            "Total Timesteps: 277942 Episode Num: 290 Reward: 1498.1351534227283\n",
            "Total Timesteps: 278942 Episode Num: 291 Reward: 1628.634839290063\n",
            "Total Timesteps: 279942 Episode Num: 292 Reward: 1606.3698205054143\n",
            "Total Timesteps: 280942 Episode Num: 293 Reward: 1551.59083504602\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1009.494030\n",
            "---------------------------------------\n",
            "Total Timesteps: 281942 Episode Num: 294 Reward: 1479.8081053712583\n",
            "Total Timesteps: 282942 Episode Num: 295 Reward: 1500.0214141546135\n",
            "Total Timesteps: 283942 Episode Num: 296 Reward: 1492.6876165395406\n",
            "Total Timesteps: 284942 Episode Num: 297 Reward: 1398.1304565687199\n",
            "Total Timesteps: 285942 Episode Num: 298 Reward: 1284.3328687755925\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 945.445749\n",
            "---------------------------------------\n",
            "Total Timesteps: 286942 Episode Num: 299 Reward: 763.1407724475747\n",
            "Total Timesteps: 287942 Episode Num: 300 Reward: 1156.0902845757237\n",
            "Total Timesteps: 288942 Episode Num: 301 Reward: 1287.7536433543762\n",
            "Total Timesteps: 289942 Episode Num: 302 Reward: 1375.4740980166541\n",
            "Total Timesteps: 290942 Episode Num: 303 Reward: 1453.3604066349885\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1402.904734\n",
            "---------------------------------------\n",
            "Total Timesteps: 291942 Episode Num: 304 Reward: 1128.709220599991\n",
            "Total Timesteps: 292942 Episode Num: 305 Reward: 1705.088331650553\n",
            "Total Timesteps: 293942 Episode Num: 306 Reward: 1813.724975619441\n",
            "Total Timesteps: 294942 Episode Num: 307 Reward: 1689.6628185571117\n",
            "Total Timesteps: 295942 Episode Num: 308 Reward: 1822.4271482189827\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1843.953345\n",
            "---------------------------------------\n",
            "Total Timesteps: 296942 Episode Num: 309 Reward: 1836.3913272116017\n",
            "Total Timesteps: 297942 Episode Num: 310 Reward: 1665.4863834284847\n",
            "Total Timesteps: 298942 Episode Num: 311 Reward: 1726.598870050816\n",
            "Total Timesteps: 299942 Episode Num: 312 Reward: 1855.5205718520347\n",
            "Total Timesteps: 300942 Episode Num: 313 Reward: 1966.698122913675\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1784.187114\n",
            "---------------------------------------\n",
            "Total Timesteps: 301942 Episode Num: 314 Reward: 1791.2357321642958\n",
            "Total Timesteps: 302942 Episode Num: 315 Reward: 1965.2983917371375\n",
            "Total Timesteps: 303942 Episode Num: 316 Reward: 1214.6891321729504\n",
            "Total Timesteps: 304942 Episode Num: 317 Reward: 2014.0021270155917\n",
            "Total Timesteps: 305942 Episode Num: 318 Reward: 1758.0817615152896\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1997.431052\n",
            "---------------------------------------\n",
            "Total Timesteps: 306942 Episode Num: 319 Reward: 1955.9399817568094\n",
            "Total Timesteps: 307942 Episode Num: 320 Reward: 1993.9803958818156\n",
            "Total Timesteps: 308942 Episode Num: 321 Reward: 1883.5547316956342\n",
            "Total Timesteps: 309942 Episode Num: 322 Reward: 1988.4721617199627\n",
            "Total Timesteps: 310942 Episode Num: 323 Reward: 1971.8250531091076\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1791.165981\n",
            "---------------------------------------\n",
            "Total Timesteps: 311942 Episode Num: 324 Reward: 1863.4502256437245\n",
            "Total Timesteps: 312942 Episode Num: 325 Reward: 1974.1412691652615\n",
            "Total Timesteps: 313942 Episode Num: 326 Reward: 2033.425642778601\n",
            "Total Timesteps: 314942 Episode Num: 327 Reward: 895.9867978534528\n",
            "Total Timesteps: 315942 Episode Num: 328 Reward: 1706.6913899694457\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1680.828052\n",
            "---------------------------------------\n",
            "Total Timesteps: 316942 Episode Num: 329 Reward: 1894.8023310413987\n",
            "Total Timesteps: 317942 Episode Num: 330 Reward: 1849.0044979023041\n",
            "Total Timesteps: 318942 Episode Num: 331 Reward: 1809.0249806069833\n",
            "Total Timesteps: 319942 Episode Num: 332 Reward: 1871.2141083654524\n",
            "Total Timesteps: 320942 Episode Num: 333 Reward: 1927.9547547594243\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1895.513180\n",
            "---------------------------------------\n",
            "Total Timesteps: 321942 Episode Num: 334 Reward: 1706.864251012048\n",
            "Total Timesteps: 322942 Episode Num: 335 Reward: 1942.1424289245056\n",
            "Total Timesteps: 323942 Episode Num: 336 Reward: 2028.333076305292\n",
            "Total Timesteps: 324942 Episode Num: 337 Reward: 1922.9102752865656\n",
            "Total Timesteps: 325942 Episode Num: 338 Reward: 1907.561967231263\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1703.939351\n",
            "---------------------------------------\n",
            "Total Timesteps: 326942 Episode Num: 339 Reward: 1751.274300693934\n",
            "Total Timesteps: 327942 Episode Num: 340 Reward: 2121.1950346402464\n",
            "Total Timesteps: 328942 Episode Num: 341 Reward: 2076.039118604863\n",
            "Total Timesteps: 329942 Episode Num: 342 Reward: 1567.0552319091478\n",
            "Total Timesteps: 330942 Episode Num: 343 Reward: 1480.0843025902477\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2034.945058\n",
            "---------------------------------------\n",
            "Total Timesteps: 331942 Episode Num: 344 Reward: 2059.236511626467\n",
            "Total Timesteps: 332942 Episode Num: 345 Reward: 1616.5427607353197\n",
            "Total Timesteps: 333942 Episode Num: 346 Reward: 1849.078128935051\n",
            "Total Timesteps: 334942 Episode Num: 347 Reward: 2031.7200682828316\n",
            "Total Timesteps: 335942 Episode Num: 348 Reward: 2106.260085588599\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2043.125744\n",
            "---------------------------------------\n",
            "Total Timesteps: 336942 Episode Num: 349 Reward: 2044.1596231508197\n",
            "Total Timesteps: 337942 Episode Num: 350 Reward: 2099.4917443017284\n",
            "Total Timesteps: 338942 Episode Num: 351 Reward: 2163.7313533261745\n",
            "Total Timesteps: 339942 Episode Num: 352 Reward: 2070.3759208450783\n",
            "Total Timesteps: 340942 Episode Num: 353 Reward: 2101.1475657272226\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1960.838107\n",
            "---------------------------------------\n",
            "Total Timesteps: 341942 Episode Num: 354 Reward: 1997.4807574004792\n",
            "Total Timesteps: 342942 Episode Num: 355 Reward: 1896.9484521809006\n",
            "Total Timesteps: 343942 Episode Num: 356 Reward: 2126.260431873653\n",
            "Total Timesteps: 344942 Episode Num: 357 Reward: 2222.7100060871117\n",
            "Total Timesteps: 345942 Episode Num: 358 Reward: 2098.8337394239375\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2174.892609\n",
            "---------------------------------------\n",
            "Total Timesteps: 346942 Episode Num: 359 Reward: 2135.381336141668\n",
            "Total Timesteps: 347942 Episode Num: 360 Reward: 2216.8101070384\n",
            "Total Timesteps: 348942 Episode Num: 361 Reward: 2191.285086150625\n",
            "Total Timesteps: 349942 Episode Num: 362 Reward: 2098.9490631198596\n",
            "Total Timesteps: 350942 Episode Num: 363 Reward: 2170.607660052658\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1663.759644\n",
            "---------------------------------------\n",
            "Total Timesteps: 351942 Episode Num: 364 Reward: 1029.6517069763029\n",
            "Total Timesteps: 352942 Episode Num: 365 Reward: 2039.441467177196\n",
            "Total Timesteps: 353942 Episode Num: 366 Reward: 2051.0141562685008\n",
            "Total Timesteps: 354942 Episode Num: 367 Reward: 2139.5387537472384\n",
            "Total Timesteps: 355942 Episode Num: 368 Reward: 2160.976279095028\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2170.311234\n",
            "---------------------------------------\n",
            "Total Timesteps: 356942 Episode Num: 369 Reward: 2178.8917660707966\n",
            "Total Timesteps: 357942 Episode Num: 370 Reward: 2078.172590904966\n",
            "Total Timesteps: 358942 Episode Num: 371 Reward: 1953.064908147138\n",
            "Total Timesteps: 359942 Episode Num: 372 Reward: 2131.6323833907363\n",
            "Total Timesteps: 360942 Episode Num: 373 Reward: 2226.2593957588715\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2218.947712\n",
            "---------------------------------------\n",
            "Total Timesteps: 361942 Episode Num: 374 Reward: 2191.009790529762\n",
            "Total Timesteps: 362942 Episode Num: 375 Reward: 2181.427294926019\n",
            "Total Timesteps: 363942 Episode Num: 376 Reward: 2129.5196192839826\n",
            "Total Timesteps: 364942 Episode Num: 377 Reward: 2159.7554079634374\n",
            "Total Timesteps: 365942 Episode Num: 378 Reward: 2176.1082050394684\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2192.597400\n",
            "---------------------------------------\n",
            "Total Timesteps: 366942 Episode Num: 379 Reward: 2160.0521184443824\n",
            "Total Timesteps: 367942 Episode Num: 380 Reward: 2185.9776270221437\n",
            "Total Timesteps: 368942 Episode Num: 381 Reward: 2172.9330792600294\n",
            "Total Timesteps: 369942 Episode Num: 382 Reward: 2201.5461549277043\n",
            "Total Timesteps: 370942 Episode Num: 383 Reward: 2160.869807698454\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2039.278889\n",
            "---------------------------------------\n",
            "Total Timesteps: 371942 Episode Num: 384 Reward: 1978.8928777896242\n",
            "Total Timesteps: 372942 Episode Num: 385 Reward: 2224.4149037070247\n",
            "Total Timesteps: 373942 Episode Num: 386 Reward: 2241.1706490696724\n",
            "Total Timesteps: 374942 Episode Num: 387 Reward: 2195.442591334982\n",
            "Total Timesteps: 375942 Episode Num: 388 Reward: 1957.8836540159423\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2279.762833\n",
            "---------------------------------------\n",
            "Total Timesteps: 376942 Episode Num: 389 Reward: 2272.6881443259795\n",
            "Total Timesteps: 377942 Episode Num: 390 Reward: 2253.785113387767\n",
            "Total Timesteps: 378942 Episode Num: 391 Reward: 2224.609588181984\n",
            "Total Timesteps: 379942 Episode Num: 392 Reward: 2222.0712857609924\n",
            "Total Timesteps: 380942 Episode Num: 393 Reward: 2194.8139313873467\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2288.244576\n",
            "---------------------------------------\n",
            "Total Timesteps: 381942 Episode Num: 394 Reward: 2250.02509589301\n",
            "Total Timesteps: 382942 Episode Num: 395 Reward: 1839.6137153922555\n",
            "Total Timesteps: 383942 Episode Num: 396 Reward: 2123.458368400593\n",
            "Total Timesteps: 384942 Episode Num: 397 Reward: 2203.008685936665\n",
            "Total Timesteps: 385942 Episode Num: 398 Reward: 2222.6885388028977\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2164.616447\n",
            "---------------------------------------\n",
            "Total Timesteps: 386942 Episode Num: 399 Reward: 2144.4857371366597\n",
            "Total Timesteps: 387942 Episode Num: 400 Reward: 2249.321265582063\n",
            "Total Timesteps: 388942 Episode Num: 401 Reward: 2251.858143815695\n",
            "Total Timesteps: 389942 Episode Num: 402 Reward: 2286.9450033820635\n",
            "Total Timesteps: 390942 Episode Num: 403 Reward: 2319.5587231476466\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2379.692095\n",
            "---------------------------------------\n",
            "Total Timesteps: 391942 Episode Num: 404 Reward: 2298.1809654049785\n",
            "Total Timesteps: 392942 Episode Num: 405 Reward: 2253.948974234377\n",
            "Total Timesteps: 393942 Episode Num: 406 Reward: 2275.696986867407\n",
            "Total Timesteps: 394942 Episode Num: 407 Reward: 2220.936407870772\n",
            "Total Timesteps: 395942 Episode Num: 408 Reward: 2094.8239286550292\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2372.008725\n",
            "---------------------------------------\n",
            "Total Timesteps: 396942 Episode Num: 409 Reward: 2364.7804227114734\n",
            "Total Timesteps: 397942 Episode Num: 410 Reward: 2092.659458045984\n",
            "Total Timesteps: 398942 Episode Num: 411 Reward: 2142.9046790033926\n",
            "Total Timesteps: 399942 Episode Num: 412 Reward: 2196.8975790789027\n",
            "Total Timesteps: 400942 Episode Num: 413 Reward: 2294.5936927176003\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2326.976762\n",
            "---------------------------------------\n",
            "Total Timesteps: 401942 Episode Num: 414 Reward: 2282.5636574764567\n",
            "Total Timesteps: 402942 Episode Num: 415 Reward: 2240.4419300445443\n",
            "Total Timesteps: 403942 Episode Num: 416 Reward: 2378.3014734808107\n",
            "Total Timesteps: 404942 Episode Num: 417 Reward: 2276.928541341027\n",
            "Total Timesteps: 405942 Episode Num: 418 Reward: 2398.3460299234926\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2433.877412\n",
            "---------------------------------------\n",
            "Total Timesteps: 406942 Episode Num: 419 Reward: 2386.62615499802\n",
            "Total Timesteps: 407942 Episode Num: 420 Reward: 2300.3947508236074\n",
            "Total Timesteps: 408942 Episode Num: 421 Reward: 2299.648587076337\n",
            "Total Timesteps: 409942 Episode Num: 422 Reward: 2306.534778341474\n",
            "Total Timesteps: 410942 Episode Num: 423 Reward: 2280.449272833021\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2396.886261\n",
            "---------------------------------------\n",
            "Total Timesteps: 411942 Episode Num: 424 Reward: 2359.7868472405507\n",
            "Total Timesteps: 412942 Episode Num: 425 Reward: 2258.1961773611188\n",
            "Total Timesteps: 413942 Episode Num: 426 Reward: 1516.1601254960872\n",
            "Total Timesteps: 414942 Episode Num: 427 Reward: 2326.991295082392\n",
            "Total Timesteps: 415942 Episode Num: 428 Reward: 2346.061676480706\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2165.001185\n",
            "---------------------------------------\n",
            "Total Timesteps: 416942 Episode Num: 429 Reward: 2080.817961733692\n",
            "Total Timesteps: 417942 Episode Num: 430 Reward: 2342.1126297876135\n",
            "Total Timesteps: 418942 Episode Num: 431 Reward: 2142.660462805601\n",
            "Total Timesteps: 419942 Episode Num: 432 Reward: 2240.6185566735394\n",
            "Total Timesteps: 420942 Episode Num: 433 Reward: 2244.0256801918777\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2256.055366\n",
            "---------------------------------------\n",
            "Total Timesteps: 421942 Episode Num: 434 Reward: 2200.165226521433\n",
            "Total Timesteps: 422942 Episode Num: 435 Reward: 2374.506148009114\n",
            "Total Timesteps: 423942 Episode Num: 436 Reward: 2374.690617882932\n",
            "Total Timesteps: 424942 Episode Num: 437 Reward: 2396.3793951729936\n",
            "Total Timesteps: 425942 Episode Num: 438 Reward: 2361.110941324068\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2430.767465\n",
            "---------------------------------------\n",
            "Total Timesteps: 426942 Episode Num: 439 Reward: 2429.3287617873275\n",
            "Total Timesteps: 427942 Episode Num: 440 Reward: 2278.415917486544\n",
            "Total Timesteps: 428942 Episode Num: 441 Reward: 2471.0097350466067\n",
            "Total Timesteps: 429942 Episode Num: 442 Reward: 2350.321231012645\n",
            "Total Timesteps: 430942 Episode Num: 443 Reward: 2358.2111895966204\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2435.415943\n",
            "---------------------------------------\n",
            "Total Timesteps: 431942 Episode Num: 444 Reward: 2441.228136627632\n",
            "Total Timesteps: 432942 Episode Num: 445 Reward: 2412.5503540919112\n",
            "Total Timesteps: 433942 Episode Num: 446 Reward: 1682.2241125558019\n",
            "Total Timesteps: 434942 Episode Num: 447 Reward: 2308.1322471389053\n",
            "Total Timesteps: 435942 Episode Num: 448 Reward: 2399.5565845762235\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2480.030564\n",
            "---------------------------------------\n",
            "Total Timesteps: 436942 Episode Num: 449 Reward: 2406.658006406214\n",
            "Total Timesteps: 437942 Episode Num: 450 Reward: 2424.3793021102797\n",
            "Total Timesteps: 438942 Episode Num: 451 Reward: 2415.34065634325\n",
            "Total Timesteps: 439942 Episode Num: 452 Reward: 2384.159509276493\n",
            "Total Timesteps: 440942 Episode Num: 453 Reward: 2380.017564483297\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2472.943829\n",
            "---------------------------------------\n",
            "Total Timesteps: 441942 Episode Num: 454 Reward: 2427.171058983068\n",
            "Total Timesteps: 442942 Episode Num: 455 Reward: 2404.1762471093493\n",
            "Total Timesteps: 443942 Episode Num: 456 Reward: 2399.0233550377206\n",
            "Total Timesteps: 444942 Episode Num: 457 Reward: 2399.032320374807\n",
            "Total Timesteps: 445942 Episode Num: 458 Reward: 2412.5048829898415\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2350.399483\n",
            "---------------------------------------\n",
            "Total Timesteps: 446942 Episode Num: 459 Reward: 2314.451996270143\n",
            "Total Timesteps: 447942 Episode Num: 460 Reward: 2394.1178301640293\n",
            "Total Timesteps: 448942 Episode Num: 461 Reward: 2425.134718444838\n",
            "Total Timesteps: 449942 Episode Num: 462 Reward: 2470.888706346806\n",
            "Total Timesteps: 450942 Episode Num: 463 Reward: 2413.434990915836\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2494.738731\n",
            "---------------------------------------\n",
            "Total Timesteps: 451942 Episode Num: 464 Reward: 2468.5167257025905\n",
            "Total Timesteps: 452942 Episode Num: 465 Reward: 2448.3313077491944\n",
            "Total Timesteps: 453942 Episode Num: 466 Reward: 2407.419990754026\n",
            "Total Timesteps: 454942 Episode Num: 467 Reward: 2458.4990508612245\n",
            "Total Timesteps: 455942 Episode Num: 468 Reward: 2427.2638582331947\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2494.749105\n",
            "---------------------------------------\n",
            "Total Timesteps: 456942 Episode Num: 469 Reward: 2467.049820388334\n",
            "Total Timesteps: 457942 Episode Num: 470 Reward: 2409.7803374707632\n",
            "Total Timesteps: 458942 Episode Num: 471 Reward: 2475.991670180766\n",
            "Total Timesteps: 459942 Episode Num: 472 Reward: 2490.3534576534153\n",
            "Total Timesteps: 460942 Episode Num: 473 Reward: 2383.9048705758864\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2467.912175\n",
            "---------------------------------------\n",
            "Total Timesteps: 461942 Episode Num: 474 Reward: 2422.76177444853\n",
            "Total Timesteps: 462942 Episode Num: 475 Reward: 2432.203438263849\n",
            "Total Timesteps: 463942 Episode Num: 476 Reward: 2439.7937638593785\n",
            "Total Timesteps: 464942 Episode Num: 477 Reward: 2391.869838137024\n",
            "Total Timesteps: 465942 Episode Num: 478 Reward: 2481.2387552213013\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2461.533645\n",
            "---------------------------------------\n",
            "Total Timesteps: 466942 Episode Num: 479 Reward: 2452.7671816837565\n",
            "Total Timesteps: 467942 Episode Num: 480 Reward: 2466.450346906046\n",
            "Total Timesteps: 468942 Episode Num: 481 Reward: 2479.5261801391102\n",
            "Total Timesteps: 469942 Episode Num: 482 Reward: 2497.7212319833075\n",
            "Total Timesteps: 470942 Episode Num: 483 Reward: 2384.736858635366\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2507.682597\n",
            "---------------------------------------\n",
            "Total Timesteps: 471942 Episode Num: 484 Reward: 2479.2932723973026\n",
            "Total Timesteps: 472942 Episode Num: 485 Reward: 2450.9545094779605\n",
            "Total Timesteps: 473942 Episode Num: 486 Reward: 2456.487685941879\n",
            "Total Timesteps: 474942 Episode Num: 487 Reward: 2452.03766500517\n",
            "Total Timesteps: 475942 Episode Num: 488 Reward: 2481.976339033992\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2445.053393\n",
            "---------------------------------------\n",
            "Total Timesteps: 476942 Episode Num: 489 Reward: 2398.477555796285\n",
            "Total Timesteps: 477942 Episode Num: 490 Reward: 2403.7716909143164\n",
            "Total Timesteps: 478942 Episode Num: 491 Reward: 2467.774368682971\n",
            "Total Timesteps: 479942 Episode Num: 492 Reward: 2423.3311602720055\n",
            "Total Timesteps: 480942 Episode Num: 493 Reward: 2482.335391736876\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2509.663373\n",
            "---------------------------------------\n",
            "Total Timesteps: 481942 Episode Num: 494 Reward: 2450.9603023295335\n",
            "Total Timesteps: 482942 Episode Num: 495 Reward: 2498.016992953686\n",
            "Total Timesteps: 483942 Episode Num: 496 Reward: 2428.4851236910017\n",
            "Total Timesteps: 484942 Episode Num: 497 Reward: 2463.6763789696265\n",
            "Total Timesteps: 485942 Episode Num: 498 Reward: 2487.600720152885\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2537.625014\n",
            "---------------------------------------\n",
            "Total Timesteps: 486942 Episode Num: 499 Reward: 2503.1200665903943\n",
            "Total Timesteps: 487942 Episode Num: 500 Reward: 2505.7543413907456\n",
            "Total Timesteps: 488942 Episode Num: 501 Reward: 2458.3811160524065\n",
            "Total Timesteps: 489942 Episode Num: 502 Reward: 2461.774281717004\n",
            "Total Timesteps: 490942 Episode Num: 503 Reward: 2480.3911699310506\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2490.534850\n",
            "---------------------------------------\n",
            "Total Timesteps: 491942 Episode Num: 504 Reward: 2465.058053977049\n",
            "Total Timesteps: 492942 Episode Num: 505 Reward: 2547.3624682076593\n",
            "Total Timesteps: 493942 Episode Num: 506 Reward: 2472.824338474158\n",
            "Total Timesteps: 494942 Episode Num: 507 Reward: 2448.61466727568\n",
            "Total Timesteps: 495942 Episode Num: 508 Reward: 2448.2546266622576\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2440.179173\n",
            "---------------------------------------\n",
            "Total Timesteps: 496942 Episode Num: 509 Reward: 2336.055113368475\n",
            "Total Timesteps: 497942 Episode Num: 510 Reward: 2489.156880825203\n",
            "Total Timesteps: 498942 Episode Num: 511 Reward: 2432.284458899734\n",
            "Total Timesteps: 499942 Episode Num: 512 Reward: 2489.744000145605\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2525.182791\n",
            "---------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor, self).__init__()\n",
        "        self.layer_1 = nn.Linear(state_dim, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, action_dim)\n",
        "        self.max_action = max_action\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer_1(x))\n",
        "        x = F.relu(self.layer_2(x))\n",
        "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "        return x\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "\n",
        "        super(Critic, self).__init__()\n",
        "        # Defining the first Critic neural network\n",
        "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.layer_2 = nn.Linear(400, 300)\n",
        "        self.layer_3 = nn.Linear(300, 1)\n",
        "\n",
        "        # Defining the second Critic neural network\n",
        "        self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "        self.layer_5 = nn.Linear(400, 300)\n",
        "        self.layer_6 = nn.Linear(300, 1)\n",
        "\n",
        "    def forward(self, x, u):\n",
        "\n",
        "        xu = torch.cat([x, u], 1)\n",
        "\n",
        "        # Forward-Propagation on the first Critic Neural Network\n",
        "        x1 = F.relu(self.layer_1(xu))\n",
        "        x1 = F.relu(self.layer_2(x1))\n",
        "        x1 = self.layer_3(x1)\n",
        "\n",
        "        # Forward-Propagation on the second Critic Neural Network\n",
        "        x2 = F.relu(self.layer_4(xu))\n",
        "        x2 = F.relu(self.layer_5(x2))\n",
        "        x2 = self.layer_6(x2)\n",
        "\n",
        "        return x1, x2\n",
        "\n",
        "    def Q1(self, x, u):\n",
        "\n",
        "        xu = torch.cat([x, u], 1)\n",
        "        x1 = F.relu(self.layer_1(xu))\n",
        "        x1 = F.relu(self.layer_2(x1))\n",
        "        x1 = self.layer_3(x1)\n",
        "\n",
        "        return x1\n",
        "\n",
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Building the whole Training Process into a class\n",
        "class TD3(object):\n",
        "\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "\n",
        "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
        "        self.critic = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
        "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "\n",
        "        for it in range(iterations):\n",
        "\n",
        "            # Step 4: We sample a batch of transitions (s, s’, a, r) from the memory\n",
        "            batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "            state = torch.Tensor(batch_states).to(device)\n",
        "            next_state = torch.Tensor(batch_next_states).to(device)\n",
        "            action = torch.Tensor(batch_actions).to(device)\n",
        "            reward = torch.Tensor(batch_rewards).to(device)\n",
        "            done = torch.Tensor(batch_dones).to(device)\n",
        "\n",
        "            # Step 5: From the next state s’, the Actor target plays the next action a’\n",
        "            next_action = self.actor_target(next_state)\n",
        "\n",
        "            # Step 6: We add Gaussian noise to this next action a’ and we clamp it in a range of values supported by the environment\n",
        "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
        "            noise = noise.clamp(-noise_clip, noise_clip)\n",
        "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Step 7: The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "            # Step 8: We keep the minimum of these two Q-values: min(Qt1, Qt2)\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "\n",
        "            # Step 9: We get the final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "            target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "\n",
        "            # Step 10: The two Critic models take each the couple (s, a) as input and return two Q-values Q1(s,a) and Q2(s,a) as outputs\n",
        "            current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "            # Step 11: We compute the loss coming from the two Critic models: Critic Loss = MSE_Loss(Q1(s,a), Qt) + MSE_Loss(Q2(s,a), Qt)\n",
        "            critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "            # Step 12: We backpropagate this Critic loss and update the parameters of the two Critic models with a SGD optimizer\n",
        "            self.critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic_optimizer.step()\n",
        "\n",
        "            # Step 13: Once every two iterations, we update our Actor model by performing gradient ascent on the output of the first Critic model\n",
        "            if it % policy_freq == 0:\n",
        "\n",
        "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "                self.actor_optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor_optimizer.step()\n",
        "\n",
        "                # Step 14: Still once every two iterations, we update the weights of the Actor target by polyak averaging\n",
        "                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                # Step 15: Still once every two iterations, we update the weights of the Critic target by polyak averaging\n",
        "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    # Making a save method to save a trained model\n",
        "    def save(self, filename, directory):\n",
        "\n",
        "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "\n",
        "    # Making a load method to load a pre-trained model\n",
        "    def load(self, filename, directory):\n",
        "\n",
        "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))\n",
        "\n",
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "    avg_reward = 0.\n",
        "    for _ in range(eval_episodes):\n",
        "\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            action = policy.select_action(np.array(obs))\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            avg_reward += reward\n",
        "\n",
        "    avg_reward /= eval_episodes\n",
        "    print (\"---------------------------------------\")\n",
        "    print (\"Average Reward over the Evaluation Step: %f\" % (avg_reward))\n",
        "    print (\"---------------------------------------\")\n",
        "\n",
        "    return avg_reward\n",
        "\n",
        "env_name = \"AntBulletEnv-v0\"\n",
        "seed = 0\n",
        "\n",
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")\n",
        "\n",
        "eval_episodes = 10\n",
        "save_env_vid = True\n",
        "env = gym.make(env_name)\n",
        "max_episode_steps = env._max_episode_steps\n",
        "\n",
        "if save_env_vid:\n",
        "    env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "    env.reset()\n",
        "\n",
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])\n",
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "policy.load(file_name, './pytorch_models/')\n",
        "_ = evaluate_policy(policy, eval_episodes=eval_episodes)"
      ],
      "metadata": {
        "id": "JeA3kfTBUnQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}